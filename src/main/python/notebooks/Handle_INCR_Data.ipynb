{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atlantic-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earned-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "absolute-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "italian-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-budget",
   "metadata": {},
   "source": [
    "#### Create Spark Session and Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "final-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName('Handle_INCR_Data'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "popular-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "increasing-accordance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-------+-------------+\n",
      "| id|name| sal|address|modified_date|\n",
      "+---+----+----+-------+-------------+\n",
      "|  1| ABC|5000|    Hyd|   2015-01-12|\n",
      "|  2| DEF|4000|   Bang|   2016-03-15|\n",
      "|  3| GHI|3000|   Pune|   2014-06-18|\n",
      "|  4| JKL|4500|    Chn|   2018-01-03|\n",
      "|  5| MNO|5600|    Chn|   2019-04-17|\n",
      "+---+----+----+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_df = spark.read.csv('/home/rameshbabug/Documents/projects/internal/spark-playground/src/data/hist.csv', header=True)\n",
    "hist_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "seven-element",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-------+-------------+----+\n",
      "| id|name| sal|address|modified_date|flag|\n",
      "+---+----+----+-------+-------------+----+\n",
      "|  2| DEF|4500|   Bang|   2017-05-15|   U|\n",
      "|  3| GHI|3000|    Hyd|   2019-03-18|   U|\n",
      "|  4| JKL|4500|    Chn|   2019-01-03|   D|\n",
      "| 10|ABC1|5300|    Hyd|   2020-01-05|   D|\n",
      "|  8| XYZ|4800|    Chn|   2020-10-21|   I|\n",
      "|  9| PQR|5000|   Bang|   2020-12-29|   I|\n",
      "+---+----+----+-------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incr_df = spark.read.csv('/home/rameshbabug/Documents/projects/internal/spark-playground/src/data/incr.csv', header=True)\n",
    "incr_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-thriller",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "failing-coordination",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "british-strip",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "mature-harvard",
   "metadata": {},
   "source": [
    "### Handle Deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "presidential-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-------+-------------+----+\n",
      "| id|name| sal|address|modified_date|flag|\n",
      "+---+----+----+-------+-------------+----+\n",
      "|  4| JKL|4500|    Chn|   2019-01-03|   D|\n",
      "| 10|ABC1|5300|    Hyd|   2020-01-05|   D|\n",
      "+---+----+----+-------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delete_df = incr_df.filter(incr_df['Flag']=='D')\n",
    "delete_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hydraulic-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "confirmed-protocol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sal: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- modified_date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id_1: string (nullable = true)\n",
      " |-- name_1: string (nullable = true)\n",
      " |-- sal_1: string (nullable = true)\n",
      " |-- address_1: string (nullable = true)\n",
      " |-- modified_date_1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_df.printSchema()\n",
    "hist_df_updated = hist_df.select(*(col(x).alias(x + '_1') for x in hist_df.columns))\n",
    "hist_df_updated.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ignored-particle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sal: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- modified_date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id_2: string (nullable = true)\n",
      " |-- name_2: string (nullable = true)\n",
      " |-- sal_2: string (nullable = true)\n",
      " |-- address_2: string (nullable = true)\n",
      " |-- modified_date_2: string (nullable = true)\n",
      " |-- flag_2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delete_df.printSchema()\n",
    "delete_df_updated = delete_df.select(*(col(x).alias(x + '_2') for x in delete_df.columns))\n",
    "delete_df_updated.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pediatric-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "insured-looking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: string (nullable = true)\n",
      " |-- name_1: string (nullable = true)\n",
      " |-- sal_1: string (nullable = true)\n",
      " |-- address_1: string (nullable = true)\n",
      " |-- modified_date_1: string (nullable = true)\n",
      " |-- id_2: string (nullable = true)\n",
      " |-- name_2: string (nullable = true)\n",
      " |-- sal_2: string (nullable = true)\n",
      " |-- address_2: string (nullable = true)\n",
      " |-- modified_date_2: string (nullable = true)\n",
      " |-- flag_2: string (nullable = true)\n",
      "\n",
      "+----+------+-----+---------+---------------+----+------+-----+---------+---------------+------+\n",
      "|id_1|name_1|sal_1|address_1|modified_date_1|id_2|name_2|sal_2|address_2|modified_date_2|flag_2|\n",
      "+----+------+-----+---------+---------------+----+------+-----+---------+---------------+------+\n",
      "|   7|   VWX| 3200|      Hyd|     2019-12-24|null|  null| null|     null|           null|  null|\n",
      "|   3|   GHI| 3000|     Pune|     2014-06-18|null|  null| null|     null|           null|  null|\n",
      "|   5|   MNO| 5600|      Chn|     2019-04-17|null|  null| null|     null|           null|  null|\n",
      "|   6|   STU| 4200|     Pune|     2020-09-26|null|  null| null|     null|           null|  null|\n",
      "|   1|   ABC| 5000|      Hyd|     2015-01-12|null|  null| null|     null|           null|  null|\n",
      "|null|  null| null|     null|           null|  10|  ABC1| 5300|      Hyd|     2020-01-05|     D|\n",
      "|   4|   JKL| 4500|      Chn|     2018-01-03|   4|   JKL| 4500|      Chn|     2019-01-03|     D|\n",
      "|   2|   DEF| 4000|     Bang|     2016-03-15|null|  null| null|     null|           null|  null|\n",
      "+----+------+-----+---------+---------------+----+------+-----+---------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshot_df = hist_df_updated.join(broadcast(delete_df_updated), col('id_1') == col('id_2'), \"fullouter\")\n",
    "snapshot_df.printSchema()\n",
    "snapshot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "simple-means",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sal: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- modified_date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      "\n",
      "+---+----+----+-------+-------------+----+\n",
      "| id|name| sal|address|modified_date|flag|\n",
      "+---+----+----+-------+-------------+----+\n",
      "|  4| JKL|4500|    Chn|   2019-01-03|   D|\n",
      "+---+----+----+-------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual_delete_records_df = snapshot_df.filter(col(\"id_1\") == col(\"id_2\")).select([col(c).alias(c.replace(\"_2\", \"\")) for c in delete_df_updated.columns])\n",
    "actual_delete_records_df.printSchema()\n",
    "actual_delete_records_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "subsequent-journey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sal: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- modified_date: string (nullable = true)\n",
      "\n",
      "+---+----+----+-------+-------------+\n",
      "| id|name| sal|address|modified_date|\n",
      "+---+----+----+-------+-------------+\n",
      "|  1| ABC|5000|    Hyd|   2015-01-12|\n",
      "|  2| DEF|4000|   Bang|   2016-03-15|\n",
      "|  3| GHI|3000|   Pune|   2014-06-18|\n",
      "|  5| MNO|5600|    Chn|   2019-04-17|\n",
      "|  6| STU|4200|   Pune|   2020-09-26|\n",
      "|  7| VWX|3200|    Hyd|   2019-12-24|\n",
      "+---+----+----+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshot_df_updated = snapshot_df.filter(col(\"id_1\").isNotNull() & col(\"id_2\").isNull()).select([col(c).alias(c.replace(\"_1\", \"\")) for c in hist_df_updated.columns])\n",
    "snapshot_df_updated.printSchema()\n",
    "snapshot_df_updated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-sentence",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "detailed-crown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "prepared-heading",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cordless-revolution",
   "metadata": {},
   "source": [
    "### Handle Inserts/Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "parental-madison",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-------+-------------+----+\n",
      "| id|name| sal|address|modified_date|flag|\n",
      "+---+----+----+-------+-------------+----+\n",
      "|  2| DEF|4500|   Bang|   2017-05-15|   U|\n",
      "|  3| GHI|3000|    Hyd|   2019-03-18|   U|\n",
      "|  8| XYZ|4800|    Chn|   2020-10-21|   I|\n",
      "|  9| PQR|5000|   Bang|   2020-12-29|   I|\n",
      "+---+----+----+-------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "insert_update_df = incr_df.filter((col('flag') == 'I') | (col('flag') == 'U'))\n",
    "insert_update_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "painted-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, desc\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "difficult-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-------+-------------+\n",
      "| id|name| sal|address|modified_date|\n",
      "+---+----+----+-------+-------------+\n",
      "|  1| ABC|5000|    Hyd|   2015-01-12|\n",
      "|  2| DEF|4000|   Bang|   2016-03-15|\n",
      "|  3| GHI|3000|   Pune|   2014-06-18|\n",
      "|  5| MNO|5600|    Chn|   2019-04-17|\n",
      "|  6| STU|4200|   Pune|   2020-09-26|\n",
      "|  7| VWX|3200|    Hyd|   2019-12-24|\n",
      "|  2| DEF|4500|   Bang|   2017-05-15|\n",
      "|  3| GHI|3000|    Hyd|   2019-03-18|\n",
      "|  8| XYZ|4800|    Chn|   2020-10-21|\n",
      "|  9| PQR|5000|   Bang|   2020-12-29|\n",
      "+---+----+----+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshot_df_latest = snapshot_df_updated.select(*snapshot_df_updated.columns).union(insert_update_df.select(*snapshot_df_updated.columns))\n",
    "snapshot_df_latest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "productive-reception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-------+-------------+\n",
      "| id|name| sal|address|modified_date|\n",
      "+---+----+----+-------+-------------+\n",
      "|  7| VWX|3200|    Hyd|   2019-12-24|\n",
      "|  3| GHI|3000|    Hyd|   2019-03-18|\n",
      "|  8| XYZ|4800|    Chn|   2020-10-21|\n",
      "|  5| MNO|5600|    Chn|   2019-04-17|\n",
      "|  6| STU|4200|   Pune|   2020-09-26|\n",
      "|  9| PQR|5000|   Bang|   2020-12-29|\n",
      "|  1| ABC|5000|    Hyd|   2015-01-12|\n",
      "|  2| DEF|4500|   Bang|   2017-05-15|\n",
      "+---+----+----+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshot_df_final = snapshot_df_latest.withColumn(\"rownum\", row_number().over(Window.partitionBy('id').orderBy(desc('modified_date')))).filter(\"rownum == 1\").drop(\"rownum\")\n",
    "snapshot_df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-disco",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
