{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:27: error: object adaptive is not a member of package org.apache.spark.sql.execution\n       import org.apache.spark.sql.execution.adaptive.{AdaptiveSparkPlanExec}\n                                             ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.execution.adaptive.{AdaptiveSparkPlanExec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:25: error: not found: type SparkConf\n       val sparkconf = new SparkConf()\n                           ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "val sparkconf = new SparkConf()\n",
    "    .setMaster(\"local[*]\")\n",
    "    .setAppName(\"Dynamic Join Startegy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:27: error: not found: value sparkconf\n           .config(sparkconf)\n                   ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .config(sparkconf)\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"50MB\")\n",
    "    .config(\"spark.sql.adaptive.logLevel\", \"TRACE\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1 = [i_item_id: bigint, i_price: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[i_item_id: bigint, i_price: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.sql(\n",
    "    \"\"\"\n",
    "      |SELECT id AS i_item_id,\n",
    "      |CAST(rand() * 1000 AS INT) AS i_price\n",
    "      |FROM RANGE(30000000)\n",
    "      |\"\"\".stripMargin\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|i_item_id|i_price|\n",
      "+---------+-------+\n",
      "|        0|     15|\n",
      "|        1|    608|\n",
      "|        2|    886|\n",
      "|        3|    779|\n",
      "|        4|    459|\n",
      "+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2 = [s_item_id: int, s_quantity: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[s_item_id: int, s_quantity: int ... 1 more field]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = spark.sql(\n",
    "    \"\"\"\n",
    "      |SELECT CASE WHEN rand() < 0.8 THEN 100 ELSE CAST(rand() * 30000000 AS INT) END AS s_item_id,\n",
    "      |CAST(rand() * 100 AS INT) AS s_quantity,\n",
    "      |DATE_ADD(current_date(), - CAST(rand() * 360 AS INT)) AS s_date\n",
    "      |FROM RANGE(10000000)\n",
    "      |\"\"\".stripMargin\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+\n",
      "|s_item_id|s_quantity|    s_date|\n",
      "+---------+----------+----------+\n",
      "|      100|        78|2019-12-21|\n",
      "|      100|        32|2020-06-22|\n",
      "|      100|        85|2020-04-27|\n",
      "|      100|        76|2020-03-25|\n",
      "|      100|        23|2020-04-23|\n",
      "+---------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfAdaptive = [s_date: date, total_sales: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[s_date: date, total_sales: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfAdaptive = spark.sql(\n",
    "    \"\"\"\n",
    "      |SELECT s_date, sum(s_quantity * i_price) AS total_sales\n",
    "      |FROM sales\n",
    "      |JOIN items ON s_item_id = i_item_id\n",
    "      |WHERE i_price < 10\n",
    "      |GROUP BY s_date\n",
    "      |ORDER BY total_sales DESC\n",
    "      |\"\"\".stripMargin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "planbefore = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "*(7) Sort [total_sales#29L DESC NULLS LAST], true, 0\n",
       "+- Exchange rangepartitioning(total_sales#29L DESC NULLS LAST, 200)\n",
       "   +- *(6) HashAggregate(keys=[s_date#14], functions=[sum(cast((s_quantity#13 * i_price#1) as bigint))], output=[s_date#14, total_sales#29L])\n",
       "      +- Exchange hashpartitioning(s_date#14, 200)\n",
       "         +- *(5) HashAggregate(keys=[s_date#14], functions=[partial_sum(cast((s_quantity#13 * i_price#1) as bigint))], output=[s_date#14, sum#35L])\n",
       "            +- *(5) Project [s_quantity#13, s_date#14, i_price#1]\n",
       "               +- *(5) SortMergeJoin [cast(s_item_id#12 as bigint)], [i_item_id#0L], Inner\n",
       "                  :- *(2) Sort [cast(s_item_id#12 as bigint) ASC NULLS FIRST], false, 0\n",
       "                  :  +- Exchange ha...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val planbefore = dfAdaptive.queryExecution.executedPlan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result = Array([2019-11-07,21851], [2020-02-10,18394], [2020-05-15,18382], [2020-04-25,18055], [2019-10-20,17096], [2020-06-07,16951], [2020-03-22,16944], [2020-03-01,16891], [2019-12-19,16889], [2020-06-28,16767], [2020-04-17,16765], [2020-05-30,16684], [2020-06-03,16682], [2020-05-02,16582], [2019-09-15,16582], [2019-12-31,16509], [2019-12-07,16403], [2020-07-22,16364], [2020-03-25,16271], [2020-08-06,16267], [2020-06-22,16111], [2019-11-11,16020], [2020-07-19,15991], [2019-12-08,15985], [2019-11-13,15977], [2020-04-09,15968], [2020-07-17,15921], [2020-08-11,15915], [2019-10-21,15888], [2019-09-08,15820], [2020-03-10,15769], [2020-02-15,15746], [2020-05-31,15517], [2020-06-12,15487], [2020-05-03,15472], [2020-05-09,15443], [2019-11-10,15425], [2020-06-...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array([2019-11-07,21851], [2020-02-10,18394], [2020-05-15,18382], [2020-04-25,18055], [2019-10-20,17096], [2020-06-07,16951], [2020-03-22,16944], [2020-03-01,16891], [2019-12-19,16889], [2020-06-28,16767], [2020-04-17,16765], [2020-05-30,16684], [2020-06-03,16682], [2020-05-02,16582], [2019-09-15,16582], [2019-12-31,16509], [2019-12-07,16403], [2020-07-22,16364], [2020-03-25,16271], [2020-08-06,16267], [2020-06-22,16111], [2019-11-11,16020], [2020-07-19,15991], [2019-12-08,15985], [2019-11-13,15977], [2020-04-09,15968], [2020-07-17,15921], [2020-08-11,15915], [2019-10-21,15888], [2019-09-08,15820], [2020-03-10,15769], [2020-02-15,15746], [2020-05-31,15517], [2020-06-12,15487], [2020-05-03,15472], [2020-05-09,15443], [2019-11-10,15425], [2020-06-..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = dfAdaptive.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "planAfter = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "*(7) Sort [total_sales#29L DESC NULLS LAST], true, 0\n",
       "+- Exchange rangepartitioning(total_sales#29L DESC NULLS LAST, 200)\n",
       "   +- *(6) HashAggregate(keys=[s_date#14], functions=[sum(cast((s_quantity#13 * i_price#1) as bigint))], output=[s_date#14, total_sales#29L])\n",
       "      +- Exchange hashpartitioning(s_date#14, 200)\n",
       "         +- *(5) HashAggregate(keys=[s_date#14], functions=[partial_sum(cast((s_quantity#13 * i_price#1) as bigint))], output=[s_date#14, sum#35L])\n",
       "            +- *(5) Project [s_quantity#13, s_date#14, i_price#1]\n",
       "               +- *(5) SortMergeJoin [cast(s_item_id#12 as bigint)], [i_item_id#0L], Inner\n",
       "                  :- *(2) Sort [cast(s_item_id#12 as bigint) ASC NULLS FIRST], false, 0\n",
       "                  :  +- Exchange has...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val planAfter = dfAdaptive.queryExecution.executedPlan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAdaptive.show(100,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
